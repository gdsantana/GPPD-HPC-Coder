# Fine-tuning DeepSeek-Coder-6.7B em RTX 4090

ImplementaÃ§Ã£o otimizada para realizar fine-tuning do modelo DeepSeek-Coder-6.7B em uma GPU RTX 4090 (24GB VRAM) utilizando tÃ©cnicas avanÃ§adas de otimizaÃ§Ã£o de memÃ³ria.

## ğŸ¯ OtimizaÃ§Ãµes Implementadas

### 1. **QLoRA (4-bit Quantization)**
- QuantizaÃ§Ã£o 4-bit com double quantization (NF4)
- Reduz uso de memÃ³ria em ~75% comparado com FP16
- MantÃ©m qualidade do treinamento prÃ³xima ao full precision

### 2. **Gradient Checkpointing**
- Recomputa ativaÃ§Ãµes durante backpropagation
- Reduz memÃ³ria de ativaÃ§Ãµes significativamente
- Trade-off: ~20% mais lento, mas viabiliza o treinamento

### 3. **LoRA (Low-Rank Adaptation)**
- Apenas 0.5-2% dos parÃ¢metros sÃ£o treinÃ¡veis
- Rank configurÃ¡vel (8, 16, 32, 64)
- Treina matrizes de baixo rank ao invÃ©s do modelo completo

### 4. **Flash Attention 2 (Opcional)**
- AtenÃ§Ã£o otimizada O(N) ao invÃ©s de O(NÂ²)
- Reduz uso de memÃ³ria e aumenta velocidade
- Requer instalaÃ§Ã£o separada

### 5. **Paged Optimizers**
- Usa `paged_adamw_32bit` para estados do otimizador
- Gerencia memÃ³ria de forma mais eficiente
- Evita OOM em modelos grandes

### 6. **Gradient Accumulation**
- Simula batches maiores sem aumentar memÃ³ria
- Batch efetivo = batch_size Ã— accumulation_steps
- Exemplo: 1 Ã— 16 = batch efetivo de 16

### 7. **Mixed Precision Training**
- BF16 (preferido) ou FP16
- Reduz uso de memÃ³ria pela metade
- Acelera computaÃ§Ã£o em GPUs modernas

## ğŸ“‹ Requisitos

### Hardware
- GPU: RTX 4090 (24GB VRAM)
- RAM: MÃ­nimo 32GB recomendado
- Storage: ~50GB livre (modelo + checkpoints)

### Software
```bash
pip install -r requirements.txt
```

**Flash Attention 2 (Opcional mas Recomendado):**
```bash
pip install flash-attn --no-build-isolation
```

## ğŸš€ Uso BÃ¡sico

### Treinamento PadrÃ£o
```bash
python finetune_deepseek_optimized.py \
  --model_name deepseek-ai/deepseek-coder-6.7b-base \
  --output_dir ./results_deepseek \
  --save_dir ./trained_deepseek \
  --epochs 3 \
  --max_length 1024 \
  --per_device_train_batch_size 1 \
  --gradient_accumulation_steps 16
```

### Treinamento com Log em Arquivo
```bash
python finetune_deepseek_optimized.py \
  --model_name deepseek-ai/deepseek-coder-6.7b-base \
  --output_dir ./results_deepseek \
  --save_dir ./trained_deepseek \
  --log_file ./logs/training_$(date +%Y%m%d_%H%M%S).log
```

> **Nota:** O parÃ¢metro `--log_file` salva todo o output do treinamento (prints de debug, progresso, erros) em um arquivo, alÃ©m de exibir no terminal. O diretÃ³rio do arquivo Ã© criado automaticamente se nÃ£o existir.

### Com Flash Attention 2
```bash
python finetune_deepseek_optimized.py \
  --model_name deepseek-ai/deepseek-coder-6.7b-base \
  --use_flash_attention \
  --packing \
  --max_length 2048
```

### Teste RÃ¡pido (100 amostras)
```bash
python finetune_deepseek_optimized.py \
  --model_name deepseek-ai/deepseek-coder-6.7b-base \
  --max_samples 100 \
  --epochs 1 \
  --save_steps 50
```

## ğŸ“¦ Datasets Opcionais

AlÃ©m do dataset padrÃ£o (HPC-Instruct), vocÃª pode adicionar datasets adicionais para melhorar a capacidade de geraÃ§Ã£o de cÃ³digo:

### Datasets DisponÃ­veis

1. **Evol-Instruct-Code-80k-v1** (80k exemplos) - Dataset de cÃ³digo evolutivo
2. **Magicoder-OSS-Instruct-75K** (75k exemplos) - Dataset de instruÃ§Ãµes OSS

### Uso

```bash
# Adicionar Evol-Instruct
python finetune_deepseek_optimized.py \
  --model_name deepseek-ai/deepseek-coder-6.7b-base \
  --use_evol_instruct

# Adicionar Magicoder
python finetune_deepseek_optimized.py \
  --model_name deepseek-ai/deepseek-coder-6.7b-base \
  --use_magicoder

# Usar todos os datasets
python finetune_deepseek_optimized.py \
  --model_name deepseek-ai/deepseek-coder-6.7b-base \
  --use_evol_instruct \
  --use_magicoder
```

ğŸ“– **DocumentaÃ§Ã£o completa**: Veja [DATASET_LOADER_GUIDE.md](DATASET_LOADER_GUIDE.md) (seÃ§Ã£o "Datasets Adicionais Opcionais")

## âš™ï¸ ParÃ¢metros Principais

| ParÃ¢metro | PadrÃ£o | DescriÃ§Ã£o |
|-----------|--------|-----------|
| `--model_name` | `deepseek-ai/deepseek-coder-6.7b-base` | Modelo base |
| `--max_length` | `1024` | Comprimento mÃ¡ximo de sequÃªncia |
| `--per_device_train_batch_size` | `1` | Batch size (manter em 1) |
| `--gradient_accumulation_steps` | `16` | AcumulaÃ§Ã£o de gradiente |
| `--lora_r` | `64` | Rank do LoRA |
| `--lora_alpha` | `128` | Alpha do LoRA (2Ã— rank) |
| `--learning_rate` | `2e-4` | Taxa de aprendizado |
| `--epochs` | `3` | NÃºmero de Ã©pocas |
| `--use_flash_attention` | `False` | Habilitar Flash Attention 2 |
| `--packing` | `False` | Empacotar sequÃªncias |
| `--use_evol_instruct` | `False` | Adicionar dataset Evol-Instruct |
| `--use_magicoder` | `False` | Adicionar dataset Magicoder |
| `--log_file` | `None` | Caminho para arquivo de log (opcional) |

## ğŸ“Š Monitoramento e Logging

### Monitor de GPU em Tempo Real
```bash
python monitor_gpu.py
```

### Ver Apenas GPU
```bash
python monitor_gpu.py --no-cpu
```

### Snapshot Ãšnico
```bash
python monitor_gpu.py --once
```

### Logging de Treinamento

Todos os scripts de treinamento suportam logging dual (terminal + arquivo):

```bash
# Salvar logs com timestamp automÃ¡tico
python finetune_deepseek_optimized.py \
  --model_name deepseek-ai/deepseek-coder-6.7b-base \
  --log_file ./logs/training_$(date +%Y%m%d_%H%M%S).log

# Ou especificar caminho fixo
python finetune_with_args.py \
  --model_name deepseek-ai/deepseek-coder-1.3b-base \
  --log_file ./training.log
```

**CaracterÃ­sticas do sistema de logging:**
- âœ… Output simultÃ¢neo no terminal e arquivo
- âœ… Timestamps de inÃ­cio e fim do treinamento
- âœ… CriaÃ§Ã£o automÃ¡tica de diretÃ³rios
- âœ… Modo append (mÃºltiplos treinamentos no mesmo arquivo)
- âœ… Captura prints de debug, progresso e erros

## ğŸ›ï¸ Ajuste Fino de HiperparÃ¢metros

### Reduzir Uso de MemÃ³ria

1. **Diminuir max_length:**
```bash
--max_length 512
```

2. **LoRA com rank menor:**
```bash
--lora_r 32 --lora_alpha 64
```

3. **Aumentar gradient accumulation:**
```bash
--gradient_accumulation_steps 32
```

### Aumentar Qualidade do Treinamento

1. **LoRA com rank maior:**
```bash
--lora_r 128 --lora_alpha 256
```

2. **SequÃªncias mais longas:**
```bash
--max_length 2048 --use_flash_attention
```

3. **Batch efetivo maior:**
```bash
--gradient_accumulation_steps 32
```

## ğŸ’¾ Uso de MemÃ³ria Estimado

| ConfiguraÃ§Ã£o | MemÃ³ria GPU | Batch Efetivo | Velocidade |
|--------------|-------------|---------------|------------|
| Conservative | ~18GB | 8 | RÃ¡pido |
| Balanced | ~20GB | 16 | MÃ©dio |
| Quality | ~22GB | 32 | Lento |

**Conservative:**
```bash
--max_length 512 --lora_r 32 --gradient_accumulation_steps 8
```

**Balanced (Recomendado):**
```bash
--max_length 1024 --lora_r 64 --gradient_accumulation_steps 16
```

**Quality:**
```bash
--max_length 2048 --lora_r 128 --gradient_accumulation_steps 32 --use_flash_attention
```

## ğŸ”§ InferÃªncia com Modelo Treinado

### Usando Adaptadores LoRA
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

base_model = AutoModelForCausalLM.from_pretrained(
    "deepseek-ai/deepseek-coder-6.7b-base",
    torch_dtype=torch.float16,
    device_map="auto"
)

model = PeftModel.from_pretrained(base_model, "./trained_deepseek")
tokenizer = AutoTokenizer.from_pretrained("./trained_deepseek")

prompt = "### Instruction:\nWrite a CUDA kernel for matrix multiplication\n\n### Response:\n"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=512)
print(tokenizer.decode(outputs[0]))
```

### Usando Modelo Mesclado
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(
    "./trained_deepseek/merged_model",
    torch_dtype=torch.float16,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("./trained_deepseek/merged_model")
```

## ğŸ› Troubleshooting

### OOM (Out of Memory)
1. Reduzir `max_length` para 512 ou 256
2. Reduzir `lora_r` para 32 ou 16
3. Aumentar `gradient_accumulation_steps`
4. Fechar outros programas usando GPU

### Treinamento Muito Lento
1. Habilitar `--use_flash_attention`
2. Habilitar `--packing`
3. Aumentar `per_device_train_batch_size` se houver memÃ³ria
4. Reduzir `dataloader_num_workers`

### Erro de Flash Attention
```bash
pip uninstall flash-attn
pip install flash-attn --no-build-isolation
```
Ou remover `--use_flash_attention` da linha de comando.

### CUDA Out of Memory Durante Salvamento
```python
model.config.use_cache = False
```
JÃ¡ implementado no script. Se persistir, salvar apenas adaptadores LoRA.

## ğŸ“ˆ Benchmark de Performance

Baseado em testes com RTX 4090:

| Config | Tokens/s | MemÃ³ria | Tempo/Epoch |
|--------|----------|---------|-------------|
| Conservative | ~1200 | 18GB | 45min |
| Balanced | ~1000 | 20GB | 55min |
| Quality | ~800 | 22GB | 70min |

*Valores aproximados, variam com dataset e hardware*

## ğŸ“š Estrutura de Arquivos

### Modelo Treinado
```
trained_deepseek/
â”œâ”€â”€ adapter_config.json      # ConfiguraÃ§Ã£o LoRA
â”œâ”€â”€ adapter_model.bin         # Pesos dos adaptadores
â”œâ”€â”€ tokenizer_config.json    # Config do tokenizer
â”œâ”€â”€ tokenizer.json           # Tokenizer
â””â”€â”€ merged_model/            # Modelo completo mesclado
    â”œâ”€â”€ model.safetensors.index.json
    â”œâ”€â”€ model-00001-of-00003.safetensors
    â”œâ”€â”€ model-00002-of-00003.safetensors
    â””â”€â”€ model-00003-of-00003.safetensors
```

### Scripts e MÃ³dulos
```
TCC/
â”œâ”€â”€ finetune_deepseek_optimized.py  # Script principal otimizado
â”œâ”€â”€ finetune_with_args.py           # Script configurÃ¡vel
â”œâ”€â”€ dataset_loader.py               # ğŸ†• MÃ³dulo modular de datasets
â”œâ”€â”€ check_compatibility.py          # Verificador de sistema
â”œâ”€â”€ monitor_gpu.py                  # Monitor de GPU
â”œâ”€â”€ inference_example.py            # Script de inferÃªncia
â”œâ”€â”€ generate_prompts.py             # Gerador de prompts
â””â”€â”€ requirements.txt                # DependÃªncias
```

**Novo:** O mÃ³dulo `dataset_loader.py` fornece uma interface modular para carregar datasets. Veja [DATASET_LOADER_GUIDE.md](DATASET_LOADER_GUIDE.md) para detalhes.

## ğŸ“ ReferÃªncias

- [QLoRA Paper](https://arxiv.org/abs/2305.14314)
- [LoRA Paper](https://arxiv.org/abs/2106.09685)
- [Flash Attention 2](https://arxiv.org/abs/2307.08691)
- [DeepSeek-Coder](https://github.com/deepseek-ai/DeepSeek-Coder)

## ğŸ“ Notas

- O script usa automaticamente BF16 se a GPU suportar
- Checkpoints sÃ£o salvos a cada 250 steps
- Apenas os 2 Ãºltimos checkpoints sÃ£o mantidos para economizar espaÃ§o
- O modelo mesclado Ã© opcional mas facilita deployment
- **Novo:** Sistema de logging dual permite salvar todo o output em arquivo para anÃ¡lise posterior
- Logs incluem timestamps, configuraÃ§Ãµes, progresso e estatÃ­sticas de memÃ³ria
