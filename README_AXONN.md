# Fine-tuning Distribu√≠do com AxoNN

Este documento fornece um guia r√°pido para executar fine-tuning distribu√≠do usando AxoNN no cluster HPC.

## üöÄ Quick Start

### 1. Instalar Depend√™ncias

```bash
# Criar e ativar ambiente virtual
python3 -m venv venv_axonn
source venv_axonn/bin/activate

# Instalar PyTorch
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Instalar depend√™ncias
pip install -r requirements.txt
```

### 2. Alocar N√≥s

```bash
# Alocar 2 n√≥s com 4 GPUs cada (8 GPUs total)
salloc -p tupi -N 2 --gres=gpu:4 -J finetune-axonn -t 10:00:00
```

### 3. Executar Treinamento

#### Modo Interativo

```bash
source venv_axonn/bin/activate

mpirun -np 8 --map-by ppr:4:node python finetune_with_axonn.py \
    --model_name deepseek-ai/deepseek-coder-6.7b-base \
    --save_dir ./models/my-finetuned-model \
    --tensor_parallelism 2 \
    --data_parallelism 4 \
    --epochs 3
```

#### Modo Batch

```bash
# Editar train_axonn.sh conforme necess√°rio
sbatch train_axonn.sh

# Monitorar
squeue -u $USER
tail -f logs/slurm-<JOB_ID>.out
```

## üìã Par√¢metros Principais

### Obrigat√≥rios

- `--model_name`: Modelo do HuggingFace (ex: `deepseek-ai/deepseek-coder-6.7b-base`)
- `--save_dir`: Diret√≥rio onde salvar o modelo treinado

### Paraleliza√ß√£o

- `--tensor_parallelism`: Divide o modelo entre GPUs (padr√£o: 1)
- `--data_parallelism`: Divide os dados entre GPUs (padr√£o: 1)

**Regra:** `total_gpus = tensor_parallelism √ó data_parallelism`

### Dataset

- `--dataset_name`: Dataset principal (padr√£o: `hpcgroup/hpc-instruct`)
- `--filter_language`: Filtrar por linguagem (padr√£o: `Cuda`)
- `--use_evol_instruct`: Adicionar dataset Evol-Instruct
- `--use_magicoder`: Adicionar dataset Magicoder

### Treinamento

- `--epochs`: N√∫mero de √©pocas (padr√£o: 3)
- `--per_device_train_batch_size`: Batch size por GPU (padr√£o: 2)
- `--gradient_accumulation_steps`: Acumula√ß√£o de gradiente (padr√£o: 4)
- `--learning_rate`: Taxa de aprendizado (padr√£o: 2e-4)

### LoRA

- `--lora_r`: Rank do LoRA (padr√£o: 16)
- `--lora_alpha`: Alpha do LoRA (padr√£o: 32)
- `--lora_dropout`: Dropout do LoRA (padr√£o: 0.05)

## üìä Configura√ß√µes Recomendadas

### Modelo 1.3B - 1 N√≥ (4 GPUs)

```bash
mpirun -np 4 python finetune_with_axonn.py \
    --model_name deepseek-ai/deepseek-coder-1.3b-base \
    --save_dir ./models/deepseek-1.3b-finetuned \
    --tensor_parallelism 1 \
    --data_parallelism 4 \
    --per_device_train_batch_size 4
```

### Modelo 6.7B - 2 N√≥s (8 GPUs)

```bash
mpirun -np 8 --map-by ppr:4:node python finetune_with_axonn.py \
    --model_name deepseek-ai/deepseek-coder-6.7b-base \
    --save_dir ./models/deepseek-6.7b-finetuned \
    --tensor_parallelism 2 \
    --data_parallelism 4 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 4
```

### Modelo 13B+ - 4 N√≥s (16 GPUs)

```bash
mpirun -np 16 --map-by ppr:4:node python finetune_with_axonn.py \
    --model_name deepseek-ai/deepseek-coder-13b-base \
    --save_dir ./models/deepseek-13b-finetuned \
    --tensor_parallelism 4 \
    --data_parallelism 4 \
    --per_device_train_batch_size 1 \
    --gradient_accumulation_steps 8
```

## üîç Monitoramento

### Ver Status do Job

```bash
squeue -u $USER
```

### Ver Logs em Tempo Real

```bash
# Log do SLURM
tail -f logs/slurm-<JOB_ID>.out

# Log do treinamento
tail -f logs/training_<JOB_ID>.log
```

### Monitorar GPUs

```bash
watch -n 1 srun nvidia-smi
```

## üìÅ Estrutura de Sa√≠da

Ap√≥s o treinamento, o modelo ser√° salvo em:

```
models/my-finetuned-model/
‚îú‚îÄ‚îÄ lora_adapters/          # Adaptadores LoRA (leve, ~100MB)
‚îÇ   ‚îú‚îÄ‚îÄ adapter_config.json
‚îÇ   ‚îú‚îÄ‚îÄ adapter_model.bin
‚îÇ   ‚îî‚îÄ‚îÄ tokenizer files...
‚îî‚îÄ‚îÄ merged_model/           # Modelo completo mesclado (pesado, ~13GB para 6.7B)
    ‚îú‚îÄ‚îÄ config.json
    ‚îú‚îÄ‚îÄ pytorch_model.bin
    ‚îî‚îÄ‚îÄ tokenizer files...
```

## ‚öôÔ∏è Comandos √öteis

### Verificar N√≥s Dispon√≠veis

```bash
sinfo -p tupi
sinfo -p tupi -t idle
```

### Cancelar Job

```bash
# Modo interativo
exit

# Modo batch
scancel <JOB_ID>
```

### Liberar Aloca√ß√£o

```bash
exit  # ou Ctrl+D
```

## üêõ Troubleshooting

### Out of Memory (OOM)

Reduza o batch size:
```bash
--per_device_train_batch_size 1
--gradient_accumulation_steps 8
```

### NCCL Error

Adicione vari√°veis de ambiente:
```bash
export NCCL_DEBUG=INFO
export NCCL_IB_DISABLE=0
```

### MPI Not Found

```bash
module load openmpi/4.1.0
pip install mpi4py --no-cache-dir
```

## üìö Documenta√ß√£o Completa

Para mais detalhes, consulte:
- **[AXONN_TRAINING_GUIDE.md](AXONN_TRAINING_GUIDE.md)** - Guia completo com todos os comandos
- **[comparacao_frameworks.md](docs/comparacao_frameworks.md)** - Compara√ß√£o SFTTrainer vs AxoNN
- **[DATASET_LOADER_GUIDE.md](DATASET_LOADER_GUIDE.md)** - Como usar diferentes datasets

## üÜö AxoNN vs SFTTrainer

| Aspecto | SFTTrainer | AxoNN |
|---------|-----------|-------|
| **Facilidade** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **Escalabilidade** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Performance** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Uso recomendado** | Modelos < 7B | Modelos > 7B |

**Use AxoNN quando:**
- Modelo > 7B par√¢metros
- M√∫ltiplos n√≥s dispon√≠veis
- Necessita m√°xima performance
- Ambiente HPC/cluster

**Use SFTTrainer quando:**
- Modelo < 7B par√¢metros
- Single GPU ou poucos GPUs
- Prototipagem r√°pida
- Primeira vez fazendo fine-tuning

## üìû Suporte

Para problemas:
1. Verifique os logs em `logs/`
2. Consulte [AXONN_TRAINING_GUIDE.md](AXONN_TRAINING_GUIDE.md)
3. Documenta√ß√£o AxoNN: https://axonn.readthedocs.io
