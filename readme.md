# ğŸš€ DeepSeek-Coder Fine-tuning Project

Projeto de fine-tuning para modelos DeepSeek-Coder usando LoRA e mÃºltiplos datasets de cÃ³digo.

## ğŸ“š DocumentaÃ§Ã£o

- **[QUICKSTART.md](QUICKSTART.md)** - Guia de inÃ­cio rÃ¡pido
- **[DATASET_LOADER_GUIDE.md](DATASET_LOADER_GUIDE.md)** - Guia do mÃ³dulo de datasets

## âš¡ Quick Start

### 1. Instalar DependÃªncias
```bash
# Instalar dependÃªncias
pip install -r requirements.txt
```

### 2. Treinar Modelo (Comando Principal)
```bash
python3 finetune_with_args.py \
  --model_name deepseek-ai/deepseek-coder-1.3b-instruct \
  --output_dir ./models/gppd-hpc-cuda-coder-instruct \
  --save_dir ./checkpoints/gppd-hpc-cuda-coder-instruct \
  --use_evol_instruct \
  --use_magicoder \
  --log_file ./logs/gppd-hpc-cuda-coder-instruct.log
```

## ğŸ› ï¸ Script Principal

**`finetune_with_args.py`** - Script configurÃ¡vel para fine-tuning com suporte a mÃºltiplos datasets

### Recursos
- âœ… Suporte a mÃºltiplos datasets (HPC-Instruct, Evol-Instruct, Magicoder)
- âœ… LoRA (Low-Rank Adaptation) para fine-tuning eficiente
- âœ… QuantizaÃ§Ã£o 8-bit para economia de memÃ³ria
- âœ… Logging detalhado com timestamps
- âœ… Salvamento automÃ¡tico de adaptadores e modelo mesclado

## ğŸ“– Exemplos de Uso

### Treinamento BÃ¡sico (apenas HPC-Instruct)
```bash
python3 finetune_with_args.py \
  --model_name deepseek-ai/deepseek-coder-1.3b-instruct \
  --output_dir ./models/basic-cuda-coder \
  --save_dir ./checkpoints/basic-cuda-coder
```

### Treinamento com ConfiguraÃ§Ãµes Customizadas
```bash
python3 finetune_with_args.py \
  --model_name deepseek-ai/deepseek-coder-1.3b-instruct \
  --output_dir ./models/custom-coder \
  --save_dir ./checkpoints/custom-coder \
  --epochs 5 \
  --max_length 1024 \
  --per_device_train_batch_size 2 \
  --gradient_accumulation_steps 8 \
  --log_file ./logs/custom-training.log
```

### Monitorar GPU Durante Treinamento (se disponÃ­vel)
```bash
python monitor_gpu.py
```

## ğŸ¯ ParÃ¢metros Principais

| ParÃ¢metro | DescriÃ§Ã£o | PadrÃ£o |
|-----------|-----------|---------|
| `--model_name` | Nome/caminho do modelo base | **obrigatÃ³rio** |
| `--output_dir` | DiretÃ³rio para checkpoints | `./results` |
| `--save_dir` | DiretÃ³rio do modelo final | `./trained_model` |
| `--use_evol_instruct` | Adicionar dataset Evol-Instruct | `False` |
| `--use_magicoder` | Adicionar dataset Magicoder | `False` |
| `--log_file` | Arquivo de log | `None` |
| `--epochs` | NÃºmero de Ã©pocas | `3` |
| `--max_length` | Comprimento mÃ¡ximo de tokens | `512` |

## Estrutura de SaÃ­da

```
finetune_model-{n_params}/
â”œâ”€â”€ lora_adapters/          # Adaptadores LoRA (PEFT)
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”œâ”€â”€ adapter_model.bin
â”‚   â””â”€â”€ ...
â”œâ”€â”€ merged_model/           # Modelo completo mesclado
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ tokenizer.json
â”œâ”€â”€ tokenizer_config.json
â””â”€â”€ special_tokens_map.json
```

## ğŸ”— Links Ãšteis

- [Guia RÃ¡pido](QUICKSTART.md)
- [Guia Dataset Loader](DATASET_LOADER_GUIDE.md)
- [DeepSeek-Coder GitHub](https://github.com/deepseek-ai/DeepSeek-Coder)

## ğŸ“ LicenÃ§a

Este projeto Ã© parte de um TCC (Trabalho de ConclusÃ£o de Curso).
