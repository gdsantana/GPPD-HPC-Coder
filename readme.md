# ğŸš€ DeepSeek-Coder Fine-tuning Project

Projeto completo de fine-tuning para modelos DeepSeek-Coder com otimizaÃ§Ãµes avanÃ§adas para RTX 4090.

## ğŸ“š DocumentaÃ§Ã£o

- **[README_DEEPSEEK_FINETUNE.md](README_DEEPSEEK_FINETUNE.md)** - DocumentaÃ§Ã£o completa do projeto
- **[QUICKSTART.md](QUICKSTART.md)** - Guia de inÃ­cio rÃ¡pido
- **[DATASET_LOADER_GUIDE.md](DATASET_LOADER_GUIDE.md)** - ğŸ†• Guia do mÃ³dulo de datasets
- **[REFACTORING_NOTES.md](REFACTORING_NOTES.md)** - ğŸ†• Notas sobre refatoraÃ§Ã£o

## âš¡ Quick Start

### 1. Instalar DependÃªncias
```bash
# Criar ambiente virtual
python3 -m venv venv

# Ativar ambiente virtual
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows

# Instalar dependÃªncias
pip install -r requirements.txt
```

### 2. Verificar Compatibilidade
```bash
python check_compatibility.py
```

### 3. Treinar Modelo (ConfiguraÃ§Ã£o Recomendada)
```bash
python finetune_deepseek_optimized.py \
  --model_name deepseek-ai/deepseek-coder-6.7b-base \
  --output_dir ./results \
  --save_dir ./trained_model \
  --epochs 3 \
  --max_length 1024 \
  --lora_r 64 \
  --gradient_accumulation_steps 16 \
  --use_flash_attention \
  --packing
```

### 4. InferÃªncia
```bash
python inference_example.py \
  --adapter_path ./trained_model \
  --interactive
```

## Estrutura de DiretÃ³rios

Cada modelo fine-tuned Ã© salvo com a seguinte estrutura:

```
finetune_model-{n_params}/
â”œâ”€â”€ lora_adapters/          # Adaptadores LoRA (PEFT)
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”œâ”€â”€ adapter_model.bin
â”‚   â””â”€â”€ ...
â”œâ”€â”€ merged_model/           # Modelo completo mesclado (opcional)
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ tokenizer.json
â”œâ”€â”€ tokenizer_config.json
â””â”€â”€ special_tokens_map.json
```

## ğŸ› ï¸ Scripts Principais

| Script | DescriÃ§Ã£o |
|--------|-----------|
| `finetune_deepseek_optimized.py` | Script principal com todas as otimizaÃ§Ãµes |
| `finetune_with_args.py` | Script configurÃ¡vel via argumentos |
| `check_compatibility.py` | Verificador de compatibilidade do sistema |
| `monitor_gpu.py` | Monitor de memÃ³ria GPU em tempo real |
| `inference_example.py` | Script de inferÃªncia com modelo treinado |
| `generate_prompts.py` | Gerador de prompts em lote |
| `dataset_loader.py` | MÃ³dulo modular de datasets |

## ğŸ“– Exemplos de Uso

### Treinamento BÃ¡sico
```bash
python finetune_deepseek_optimized.py
```

### Treinamento com MÃºltiplos Datasets
```bash
python finetune_deepseek_optimized.py \
  --dataset_names hpcgroup/hpc-instruct bigcode/the-stack \
  --dataset_configs default python
```

### Monitorar GPU Durante Treinamento
```bash
python monitor_gpu.py
```

### Gerar Prompts
```bash
python generate_prompts.py \
  --model_dir ./trained_model \
  --prompts_dir ./prompts \
  --output_dir ./outputs \
  --k 5
```

## ğŸ¯ ConfiguraÃ§Ãµes Recomendadas

### Conservative (~18GB VRAM)
```bash
--max_length 512 --lora_r 32 --gradient_accumulation_steps 8
```

### Balanced (~20GB VRAM) - Recomendado
```bash
--max_length 1024 --lora_r 64 --gradient_accumulation_steps 16
```

### Quality (~22GB VRAM)
```bash
--max_length 2048 --lora_r 128 --gradient_accumulation_steps 32 --use_flash_attention
```

## ğŸ”— Links Ãšteis

- [DocumentaÃ§Ã£o Completa](README_DEEPSEEK_FINETUNE.md)
- [Guia RÃ¡pido](QUICKSTART.md)
- [Guia Dataset Loader](DATASET_LOADER_GUIDE.md)
- [DeepSeek-Coder GitHub](https://github.com/deepseek-ai/DeepSeek-Coder)

## ğŸ“ LicenÃ§a

Este projeto Ã© parte de um TCC (Trabalho de ConclusÃ£o de Curso).
