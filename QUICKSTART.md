# ðŸš€ Quick Start - DeepSeek-Coder-6.7B Fine-tuning

## Setup RÃ¡pido

```bash
chmod +x setup_environment.sh
./setup_environment.sh
```

## Verificar Compatibilidade

```bash
python check_compatibility.py
```

## Treinar (ConfiguraÃ§Ã£o Recomendada)

```bash
python finetune_deepseek_optimized.py \
  --model_name deepseek-ai/deepseek-coder-6.7b-base \
  --output_dir ./results \
  --save_dir ./model_trained \
  --epochs 3 \
  --max_length 1024 \
  --lora_r 64 \
  --lora_alpha 128 \
  --gradient_accumulation_steps 16 \
  --use_flash_attention \
  --packing
```

## Teste RÃ¡pido (100 amostras)

```bash
python finetune_deepseek_optimized.py \
  --model_name deepseek-ai/deepseek-coder-6.7b-base \
  --max_samples 100 \
  --epochs 1
```

## Monitorar GPU

```bash
python monitor_gpu.py
```

## InferÃªncia

### Com adaptadores LoRA
```bash
python inference_example.py \
  --adapter_path ./model_trained \
  --instruction "Write a CUDA kernel for vector addition"
```

### Modo interativo
```bash
python inference_example.py \
  --adapter_path ./model_trained \
  --interactive
```

### Com modelo mesclado
```bash
python inference_example.py \
  --merged_model_path ./model_trained/merged_model \
  --interactive
```

## ConfiguraÃ§Ãµes Alternativas

### Conservative (menos memÃ³ria, ~18GB)
```bash
python finetune_deepseek_optimized.py \
  --max_length 512 \
  --lora_r 32 \
  --lora_alpha 64 \
  --gradient_accumulation_steps 8
```

### High Quality (mais memÃ³ria, ~22GB)
```bash
python finetune_deepseek_optimized.py \
  --max_length 2048 \
  --lora_r 128 \
  --lora_alpha 256 \
  --gradient_accumulation_steps 32 \
  --use_flash_attention \
  --packing
```

## Estrutura de Arquivos Criados

```
TCC/
â”œâ”€â”€ finetune_deepseek_optimized.py  # Script principal de treinamento
â”œâ”€â”€ requirements.txt                 # DependÃªncias Python
â”œâ”€â”€ setup_environment.sh             # Setup automÃ¡tico
â”œâ”€â”€ check_compatibility.py           # VerificaÃ§Ã£o de sistema
â”œâ”€â”€ monitor_gpu.py                   # Monitor de memÃ³ria GPU
â”œâ”€â”€ inference_example.py             # Script de inferÃªncia
â”œâ”€â”€ README_DEEPSEEK_FINETUNE.md     # DocumentaÃ§Ã£o completa
â””â”€â”€ QUICKSTART.md                    # Este arquivo
```

## Troubleshooting

### OOM Error
1. Reduzir `--max_length 512`
2. Reduzir `--lora_r 32`
3. Aumentar `--gradient_accumulation_steps 32`

### Muito Lento
1. Adicionar `--use_flash_attention`
2. Adicionar `--packing`
3. Aumentar `--per_device_train_batch_size` (se tiver memÃ³ria)

### Flash Attention Error
```bash
pip install flash-attn --no-build-isolation
```
Ou remover `--use_flash_attention`

## Ver DocumentaÃ§Ã£o Completa

```bash
cat README_DEEPSEEK_FINETUNE.md
```
