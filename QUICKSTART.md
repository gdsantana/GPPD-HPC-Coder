# ðŸš€ Quick Start - DeepSeek-Coder Fine-tuning

## InstalaÃ§Ã£o de DependÃªncias

```bash
# Instalar dependÃªncias
pip install -r requirements.txt
```

## Comando Principal de Treinamento

Use o script `finetune_with_args.py` com o seguinte comando:

```bash
python3 finetune_with_args.py \
  --model_name deepseek-ai/deepseek-coder-1.3b-instruct \
  --output_dir ./models/gppd-hpc-cuda-coder-instruct \
  --save_dir ./checkpoints/gppd-hpc-cuda-coder-instruct \
  --use_evol_instruct \
  --use_magicoder \
  --log_file ./logs/gppd-hpc-cuda-coder-instruct.log
```

## ParÃ¢metros do Script

### ParÃ¢metros ObrigatÃ³rios
- `--model_name`: Nome/caminho do modelo base (ex.: deepseek-ai/deepseek-coder-1.3b-instruct)

### ParÃ¢metros Principais
- `--output_dir`: DiretÃ³rio para checkpoints durante o treino (padrÃ£o: ./results)
- `--save_dir`: DiretÃ³rio onde o modelo final serÃ¡ salvo (padrÃ£o: ./trained_model)
- `--log_file`: Caminho para arquivo de log (opcional)

### Datasets Adicionais
- `--use_evol_instruct`: Adiciona dataset Evol-Instruct-Code-80k-v1
- `--use_magicoder`: Adiciona dataset Magicoder-OSS-Instruct-75K

### ParÃ¢metros de ConfiguraÃ§Ã£o (opcionais)
- `--max_length`: Comprimento mÃ¡ximo de tokens (padrÃ£o: 512)
- `--epochs`: NÃºmero de Ã©pocas (padrÃ£o: 3)
- `--per_device_train_batch_size`: Batch size por dispositivo (padrÃ£o: 4)
- `--gradient_accumulation_steps`: Passos de acumulaÃ§Ã£o de gradiente (padrÃ£o: 4)
- `--dataset_name`: Nome do dataset principal (padrÃ£o: hpcgroup/hpc-instruct)
- `--filter_language`: Filtrar por linguagem (padrÃ£o: Cuda)

## Exemplos de Uso

### Treinamento BÃ¡sico (apenas HPC-Instruct)
```bash
python3 finetune_with_args.py \
  --model_name deepseek-ai/deepseek-coder-1.3b-instruct \
  --output_dir ./models/basic-cuda-coder \
  --save_dir ./checkpoints/basic-cuda-coder
```

### Treinamento com ConfiguraÃ§Ãµes Customizadas
```bash
python3 finetune_with_args.py \
  --model_name deepseek-ai/deepseek-coder-1.3b-instruct \
  --output_dir ./models/custom-coder \
  --save_dir ./checkpoints/custom-coder \
  --epochs 5 \
  --max_length 1024 \
  --per_device_train_batch_size 2 \
  --gradient_accumulation_steps 8 \
  --log_file ./logs/custom-training.log
```

### Treinamento Apenas com Evol-Instruct
```bash
python3 finetune_with_args.py \
  --model_name deepseek-ai/deepseek-coder-1.3b-instruct \
  --output_dir ./models/evol-coder \
  --save_dir ./checkpoints/evol-coder \
  --use_evol_instruct
```

## Estrutura de SaÃ­da

O modelo treinado serÃ¡ salvo com a seguinte estrutura:

```
finetune_model-{n_params}/
â”œâ”€â”€ lora_adapters/          # Adaptadores LoRA (PEFT)
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”œâ”€â”€ adapter_model.bin
â”‚   â””â”€â”€ ...
â”œâ”€â”€ merged_model/           # Modelo completo mesclado
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ tokenizer.json
â”œâ”€â”€ tokenizer_config.json
â””â”€â”€ special_tokens_map.json
```

## Monitoramento

### Ver Logs em Tempo Real
```bash
tail -f ./logs/gppd-hpc-cuda-coder-instruct.log
```

### Monitorar GPU (se disponÃ­vel)
```bash
python monitor_gpu.py
```

## Troubleshooting

### Erro de MemÃ³ria (OOM)
1. Reduzir `--max_length 256`
2. Reduzir `--per_device_train_batch_size 1`
3. Aumentar `--gradient_accumulation_steps 8`

### Treinamento Muito Lento
1. Aumentar `--per_device_train_batch_size` (se tiver memÃ³ria)
2. Reduzir `--gradient_accumulation_steps`
3. Usar modelo menor (1.3B ao invÃ©s de 6.7B)

### Erro de DependÃªncias
```bash
pip install --upgrade transformers peft trl datasets bitsandbytes
```

## Recursos do Script

- âœ… Suporte a mÃºltiplos datasets (HPC-Instruct, Evol-Instruct, Magicoder)
- âœ… Logging simultÃ¢neo no terminal e arquivo
- âœ… QuantizaÃ§Ã£o 8-bit para economia de memÃ³ria
- âœ… LoRA (Low-Rank Adaptation) para fine-tuning eficiente
- âœ… Salvamento automÃ¡tico de adaptadores e modelo mesclado
- âœ… Timestamps e debug detalhado
